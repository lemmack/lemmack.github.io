---
title: "Tissue Surveillance of Persistent Toxicants"
date: 2025-11-27T16:34:00-05:00
draft: false
---

# Introduction

Brief definitions: 
* *Persistent toxicants* are chemicals that resist breakdown and clearance in the body and can accumulate over years or decades. 
* *Total body burden* is the amount of these toxicants stored in all tissues, not just what happens to be circulating on the day you draw blood.

There's a gap in public health surveillance that I believe will eventually have to be addressed if we want the march of progress in population health to continue. Current surveillance relies heavily on biological fluids (blood serum and urine) to monitor population exposure to environmental contaminants. While these matrices effectively capture recent exposures and water-soluble compounds, they frequently fail to reflect the total body burden of persistent toxicants.

This gap exists because many critical pollutants do not remain in circulation; they sequester in specific tissue compartments over a lifetime, and they can be tricky to measure. Lipophilic compounds like PCBs and pesticides partition into adipose tissue, mercury builds up in nervous tissue, lead accumulates in the skeleton, and so on. Consequently, relying solely on fluid biomonitoring creates a pharmacokinetic blind spot, risking a significant underestimation of the toxicological drivers behind chronic diseases, and especially those with long latency periods such as neurodegenerative disorders and renal pathology.

Recent post-mortem analyses underscore the urgency of addressing this gap. [Studies have confirmed](https://doi.org/10.1021/acs.est.4c09458) the presence of per- and polyfluoroalkyl substances (PFAS a.k.a. "forever chemicals") in human brain tissue, adding quantitative evidence to growing concerns about blood-brain barrier penetration by these compounds. Furthermore, [the detection of black carbon](https://doi.org/10.1186/s12989-018-0250-8) in airway macrophages serves as a dosimeter for lifetime particulate exposure that ambient air monitors cannot replicate. These findings suggest that the body acts as a historical archive, recording exposures that fluid analysis misses entirely.

The emerging concern over microplastics provides another illustration. A [2025 study in Nature Medicine](https://doi.org/10.1038/s41591-024-03453-1) comparing autopsy samples from 2016 and 2024 found that plastic concentrations in human brain tissue increased by approximately 50% over just eight years, with brain samples containing 7 to 30 times more plastic than liver or kidney samples from the same individuals. These nanoscale polyethylene shards, largely invisible to standard fluid-based monitoring, would have gone entirely undetected without tissue analysis.

# Building a Representative Sample

A valid epidemiological criticism of autopsy-based surveillance is selection bias. In public health, we often face the "[Healthy Worker Effect](https://doi.org/10.1093/occmed/49.4.225)," where the working population is healthier than the general public. With autopsy studies, we face the inverse. The population that undergoes autopsy is inherently skewed towards the elderly, the chronically ill, and those with multiple comorbidities.

If we only sample from hospital deaths, we risk measuring the toxic burden of the sick rather than the baseline burden of the population. This "denominator problem" can lead to overestimating associations between toxicants and specific diseases if not carefully managed.

## Near-Term Strategies

In the absence of systematic tissue donation, a robust surveillance program must work with available sample sources while actively mitigating their biases.

### Sudden Accidental Deaths

One approach to reducing selection bias is to prioritize forensic autopsies of sudden, accidental deaths (e.g., motor vehicle accidents) over typical hospital autopsies. These individuals are often younger and healthier prior to the event, and their tissue burden may better approximate the general population than samples from end-stage disease. However, this population introduces its own biases: it skews male and young, concentrates geographically near urban trauma centers, and may be confounded by factors associated with accidental death (such as substance use in some motor vehicle cases). These limitations would need to be addressed through careful stratification and sensitivity analyses.

### Living Tissue Banks (Surgical Waste)

We can also move beyond post-mortem sampling by utilizing "surgical waste" from living patients.

* **Bariatric Surgery:** This provides high-volume adipose tissue samples from a living population, allowing for real-time monitoring of lipophilic compounds. However, bariatric patients represent a metabolically distinct subpopulation, and findings may not generalize to the broader population without appropriate adjustment.
* **Orthopedic Surgery:** Procedures like hip replacements provide bone fragments that can be analyzed for lead and strontium accumulation. This population skews elderly and may over-represent certain socioeconomic groups with access to elective surgery.

By stratifying samples between "disease-related deaths" and "accidental deaths," and supplementing with surgical waste where possible, researchers could control for the healthy donor effect and build a more accurate model of population-level exposure. But these strategies remain fundamentally opportunistic. Trauma deaths skew young and male; surgical waste comes from patients who can access elective procedures; hospital autopsies capture the chronically ill. Even with careful stratification, these sources cannot deliver what population-level surveillance truly requires: representative tissue samples across all demographics, geographies, and health statuses.

## The Long-Term Vision: A Donation Norm

A more ambitious approach would make posthumous tissue donation for scientific surveillance a social norm, similar to organ donation but with a lighter footprint. The infrastructure for this already exists in many countries through organ donor registries. The question is whether tissue surveillance could piggyback on these systems or establish parallel opt-out frameworks.

*Note*: An open question remains: who would actually operate such a system? The CDC has the epidemiological infrastructure but no current mandate for tissue surveillance. The EPA ran NHATS but let it lapse. An interagency body might face coordination challenges, while a public-private partnership modeled on CHAMPS could face questions about data governance. The German Environmental Specimen Bank, which has operated continuously since 1985 under the Federal Environment Agency, offers one model: a government-operated biobank with standardized protocols and regular public reporting. Determining the right institutional home may be as important as solving the technical challenges.

### The Case for Opt-Out

Countries with presumed consent for organ donation consistently achieve higher participation rates than those requiring explicit opt-in. Spain, which pioneered opt-out legislation in 1979, maintains donation rates roughly double those of opt-in countries like Germany. Austria and Belgium have seen similar results. The psychological insight is straightforward: most people are willing to donate but never get around to registering. Opt-out systems align the default with the majority preference.

Tissue surveillance for toxicological research may actually be an easier sell than organ transplantation. Organ donation involves a zero-sum tension: this heart could go to patient A or patient B, and families must grapple with the knowledge that their loved one's organs are sustaining someone else's life. Tissue surveillance involves no such tension. A few tissue samples from key organs, taken after death, contribute to an anonymous scientific archive. There is no recipient, no waitlist, no competing claims.

An opt-out system for tissue surveillance might work as follows: upon death, a standard collection protocol would obtain small tissue samples from adipose, liver, kidney, brain, and bone unless the individual had previously registered an objection or the family objects at the time of death. Samples would be anonymized and banked, with demographic and (where available) residential and occupational history attached but stripped of identifying information.

### Addressing Participation Bias

One objection deserves direct engagement: opt-out systems do not guarantee representative participation. Organ donation rates already vary by race, socioeconomic status, religion, and geography, and these disparities would likely persist in tissue surveillance. Communities with historical reasons to distrust medical institutions may opt out at higher rates, potentially reproducing the very biases the program aims to eliminate.

This is a genuine concern, but it argues for intentional outreach rather than abandoning the opt-out framework. A tissue surveillance program would need to invest in community engagement, particularly in populations underrepresented in existing biobanks. It would also need transparent governance, with clear rules about data access, privacy protections, and community benefit-sharing. The goal should be to make participation feel like a contribution to collective knowledge rather than extraction by distant institutions.

### The Precedent of Body Donation

Body donation for medical education has existed for centuries, and while participation rates remain modest, the practice is widely accepted as socially valuable. Tissue surveillance asks for far less: not the entire body for dissection, but a few grams of tissue collected through minimally invasive sampling. If we can normalize full-body donation for training purposes, normalizing tissue sampling for public health surveillance seems achievable.

The deeper question is whether we can build a culture that views the body after death as a source of knowledge for the living. Every person who dies carries within their tissues a record of the air they breathed, the water they drank, the food they ate, and the products they touched. That record currently vanishes with cremation or burial. An opt-out tissue surveillance system would preserve a fraction of that information, transforming individual deaths into contributions toward understanding the environmental drivers of disease.

This would require sustained public communication about the purpose and value of the program, along with genuine accountability for how samples are used. But if successful, it could provide what no amount of clever sampling from trauma cases and surgical waste can deliver: a true population-level archive of the human exposome.

# Minimally Invasive Tissue Sampling (MITS)

To progress public health surveillance, we must move beyond the snapshot provided by blood samples and access the archive stored in tissues. This does not require a return to widely practiced complete autopsies. The emergence of Minimally Invasive Tissue Sampling (MITS), which utilizes needle biopsies to collect samples from key organs, offers a scalable, cost-effective, and culturally acceptable alternative. MITS is currently being used mostly for post-mortem analysis of infectious cases, but in theory could be adapted to analysis of persistent toxicants with a little ingenuity.

The biggest catch is sample preservation. MITS protocols in infectious disease often rely on formalin-fixed paraffin-embedded tissue, which is excellent for histology but often hostile to trace chemical measurement. If tissue surveillance is going to work for persistent toxicants, the preservation method becomes the central engineering constraint, and it drives a lot of downstream design choices.

A dedicated 'MITS-Tox' protocol would likely require fresh-frozen samples. This introduces a cold-chain logistics challenge, as biopsy cores would need to be flash-frozen in liquid nitrogen or stored at -80°C immediately after collection. While more demanding than a jar of formalin, this infrastructure already exists in many biobanks and research hospitals; it simply needs to be standardized for the field.

By integrating toxicological screening into MITS protocols, epidemiological data collection can evolve to capture the true burden of environmental toxicity. This shift could strengthen the evidence base for associations between chronic exposure and non-communicable diseases, ultimately driving more effective environmental health policy. To be clear, I am not sure if MITS-Tox is the best solution, but it is the first one that comes to mind since it would involve repurposing a system that already exists.

## The Engineering Challenge: Chemical Purity vs. Preservation

This is the core tradeoff: pathology wants structural preservation, while exposure science needs chemical fidelity. At parts-per-billion, the preservation method can easily become the dominant signal, which is why the following details matter greatly.

To accurately quantify the body burden of a pollutant, the sample must remain chemically pristine. Formalin introduces three major sources of error.

### 1. Contamination and Leaching
In toxicology, we are often measuring parts per billion (ppb). At these concentrations, formalin fixation introduces two opposing sources of error: [contamination and leaching](https://doi.org/10.1002/etc.4709).

**Contamination (False Highs)**
Standard commercial formalin can be "dirty" from a chemical perspective.
* **Trace Metals:** Formalin buffers frequently contain trace amounts of zinc, copper, and even lead from industrial manufacturing and storage. Studies have found that copper concentrations in formalin-fixed brain tissue [can increase by approximately 37%](https://doi.org/10.1007/s10534-010-9359-4) compared to fresh-frozen samples, likely due to contamination from the fixative. If you submerge a tissue sample in this solution, you lose the ability to distinguish between the metal that was in the tissue and the metal that was in the jar.
* **Plasticizers:**  Formalin is often stored in plastic containers that leach phthalates and bisphenols. This makes it difficult to accurately measure these specific endocrine disruptors in the tissue, as background contamination from storage can overwhelm the true signal.

**Leaching (False Lows)**
The opposite problem is equally serious. Formalin is an aqueous solution, and over time, soluble elements diffuse out of the tissue and into the surrounding fluid.
* **Time-Dependent Loss:** Studies of formalin-fixed brain tissue have documented substantial leaching that increases with storage duration. Concentrations of arsenic, cadmium, magnesium, and rubidium in the formalin can [increase more than 100-fold](https://doi.org/10.1007/s12011-007-8051-1) over years of storage, representing material lost from the tissue itself.
* **Differential Effects:** The magnitude of leaching varies considerably by element. Iron concentrations in fixed brain tissue can decrease by approximately 40%, and zinc by as much as 77%, while nickel and chromium show minimal leaching. This inconsistency means there is no simple correction factor; each analyte behaves differently.
The net effect is bidirectional error that varies by compound, tissue type, and storage duration, making quantitative comparisons across studies nearly impossible.

### 2. Extraction Efficiency
To measure a toxicant, you must first extract it from the tissue matrix. Formalin works by cross-linking proteins, effectively turning the tissue into a rubbery, resistant material.
* **Incomplete Digestion:** When labs try to digest this fixed tissue to release the stored heavy metals or pesticides, the cross-linked matrix resists breakdown. This often leads to an incomplete release of the toxicant, resulting in a "false low" reading.
* **Lipid Trapping:** For lipophilic compounds (like PCBs), the hardening of the tissue can physically trap the fat droplets within the protein mesh, preventing solvents from washing the toxicants out for analysis.

### 3. The "Wet Weight" Variable
Toxicology relies on precise math: *nanograms of toxin per gram of tissue*.
Formalin alters the physical mass of the sample by dehydrating cells and changing the water content over time. A sample stored in formalin for a month will have a different weight than fresh tissue. This skews the denominator in the calculation, introducing a mathematical error margin that is unacceptable for high-precision surveillance.

### The Fresh-Frozen Necessity
To solve these quantification issues, a dedicated "MITS-Tox" protocol would likely require fresh-frozen samples. This avoids chemical contamination and preserves the native mass of the tissue. While this necessitates a cold chain (dry ice or liquid nitrogen), it is the only method I can presently think of to ensure that the number on the final lab report reflects the true burden inside the patient.

# Mapping the Total Exposome

The ultimate goal of this surveillance is to operationalize the "exposome." [First proposed](https://doi.org/10.1158/1055-9965.epi-05-0456) by cancer epidemiologist Christopher Wild in 2005, the exposome encompasses the totality of human environmental exposures from conception onwards. It is the environmental complement to the genome.

Currently, our ability to measure the exposome is unbalanced. We have excellent tools for measuring the external exposome (via air monitors and water testing) and decent tools for measuring the transient internal exposome (via blood and urine biomarkers). However, we almost entirely lack data on the *cumulative* internal exposome.

This data gap cripples our ability to draw causal links between environment and disease. If we attempt to correlate a lifetime of exposure with a chronic disease using only a blood sample taken at the time of diagnosis, we are effectively trying to reconstruct a movie from a single frame.

MITS offers one possible bridge. But regardless of the method, by banking tissue we gain access to a biological record of cumulative exposures that is shaped by ongoing processes of uptake, redistribution, metabolism, and elimination.

## From Prediction to Observation

For decades, environmental health policy has relied heavily on risk assessment modeling. Agencies estimate human risk based on factory emissions data and animal studies, mathematically predicting what the body burden *should* be.

This approach will not be sufficient forever. To continue making progress, we need to complement predictive modeling with direct empirical observation. We do not need to rely solely on estimates of whether "forever chemicals" are crossing the blood-brain barrier or whether microplastics are accumulating in the liver. With a standardized, ethically sourced MITS-Tox network (or some other solution), we could measure these burdens directly, though translating such measurements into causal understanding of disease would remain a separate challenge.

The infrastructure for this exists in our hospitals and biobanks. The technology for analysis exists in our mass spectrometry labs. All that is missing is the mandate to look where the toxicants actually hide.

# What Would Better Data Enable?

The absence of systematic tissue surveillance leaves several important questions unanswered. Without population-level data on cumulative tissue burdens, regulators struggle to set meaningful reference values for persistent compounds, since blood levels reflect only recent exposure rather than lifetime accumulation. Environmental health agencies cannot determine whether existing regulations are actually reducing human body burden or merely shifting exposure to compounds that accumulate elsewhere in the body. And researchers investigating links between chronic diseases and environmental exposures must rely on proxy measures that may systematically underestimate the relevant dose.

A functioning tissue surveillance system would enable direct measurement of whether regulatory interventions translate into reduced human body burden over time. It could identify emerging contaminants accumulating in tissues before health effects become apparent. And it could provide the denominator data needed to distinguish between compounds that pass through the body quickly and those that persist for decades.

# History and Implementations

## National Human Adipose Tissue Survey (NHATS) (1970-1989)

This program was conducted between 1970-1989 by the EPA of the United States as the main study making up the [National Human Monitoring Program](https://doi.org/10.17226/1787). It collected adipose tissue samples from surgeons and medical examiners across the U.S. to monitor the prevalence of lipophilic toxic substances. It provided critical baseline data on the body burden of persistent organic pollutants like PCBs and organochlorine pesticides. It was **discontinued in 1989** as the focus shifted to the National Health and Nutrition Examination Survey (NHANES) which relies on blood and urine samples (more convenient but missing important tissue data).

It's important to note that NHATS was not discontinued because it failed scientifically. It was discontinued in 1989 largely due to budget cuts and a bureaucratic shift toward 'easier' fluids-based monitoring.

In fact, a 1991 National Academy of Sciences report strongly recommended continuing and redesigning the program, noting that fluid monitoring was insufficient for persistent pollutants. [The 1991 NAS](https://doi.org/10.17226/1787) report explicitly concluded that "the basic structure of the NHATS is such that, even with major improvements, its ability to reflect the accumulations of toxic substances for the U.S. population would be seriously limited" and recommended a redesigned program with proper probability sampling. The report noted that funding "up to $25–50 million per year suggested by heads of other agencies could be put to good use" for comprehensive tissue monitoring, a fraction of what we spend on healthcare for the diseases these pollutants may exacerbate. That level of funding never materialized, and the program has remained dormant for over thirty years.

### The Policy Failure: Surveillance vs. Modeling

But the discontinuation of NHATS in 1989 wasn't just a budgetary decision; it was related to a philosophical shift in environmental health policy. During the 1980s and 90s, regulatory agencies like the EPA moved away from *biological surveillance* (empirical observation) and toward *risk assessment modeling* (theoretical prediction).

**The "Prediction" Fallacy:** The prevailing logic was that if we knew how much pollution was coming out of factories (emissions data) and we applied mathematical models of transport and fate, we could calculate human exposure without needing to measure it physically. This approach assumed we knew all the variables.

**Why Models Failed:** History has shown this singular reliance on modeling to be an error.
* **PFAS:** Predictive models failed to anticipate the bioaccumulation of perfluorinated compounds because the chemistry was novel and the uptake pathways were not yet understood. We only found them because we physically looked.
* **PBDEs:** Flame retardants [rose in human tissue for years](https://doi.org/10.1021/es035082g) while regulators assumed they were safe, simply because there was no surveillance net to catch the trend early.
* **Microplastics:** Environmental concentrations of micro- and nanoplastics have increased exponentially over 50 years, yet the extent of human tissue accumulation remained unknown until autopsy studies revealed concentrations in brain tissue [rising in parallel with environmental levels](https://doi.org/10.1038/s41591-024-03453-1). Blood-based biomonitoring cannot capture this burden because these particles sequester in lipid-rich tissues.

Restarting a tissue archive is an admission that models are insufficient. We cannot predict the behavior of 80,000+ industrial chemicals; we must return to the empirical standard of measuring what is actually inside us.

## German Environmental Specimen Bank for Human Tissues (ESBHum) (1985 - Present)

While NHATS was being discontinued in the United States, Germany was building what would become the most sustained human biomonitoring program in the world. The German Environmental Specimen Bank for Human Tissues (ESBHum) began pilot operations in 1979 and was [established as a permanent facility in 1985](https://doi.org/10.1007/s00103-015-2298-z), the same year NHATS entered its final phase. Nearly four decades later, it continues to operate.

The program emerged alongside Germany's first Chemicals Legislation, which came into force in 1982. This timing was intentional: [the ESBHum was designed from the start](https://doi.org/10.1016/j.ijheh.2011.10.013) as a tool for evaluating the effectiveness of chemical regulations, providing empirical data on whether policy interventions actually reduced human body burden. This tight coupling between surveillance and policy may explain its longevity.

Operationally, the ESBHum recruits approximately 480 healthy, non-occupationally exposed students aged 20-29 each year from four sampling sites across Germany (Münster, Halle, Greifswald, and Ulm). The program collects whole blood, blood plasma, and 24-hour urine samples, which are [stored at -150°C in liquid nitrogen vapor](https://doi.org/10.1016/j.ijheh.2007.01.036) to preserve chemical integrity for future retrospective analysis. This ultra-low temperature storage addresses many of the preservation challenges discussed earlier in this essay, though the program focuses primarily on biological fluids rather than solid organ tissue.

The ESBHum operates on two parallel tracks. Real-time monitoring analyzes each year's samples for approximately 25 inorganic and organic substances, generating an immediate snapshot of population exposure. Retrospective monitoring returns to the cryo-archived samples when new analytical methods become available or when previously unrecognized compounds emerge as concerns. This dual approach has proven remarkably effective at catching what would otherwise be invisible trends.

### What Sustained Surveillance Has Revealed

The ESBHum's continuous operation has generated a uniquely valuable dataset. [Retrospective analyses have documented](https://doi.org/10.1016/j.toxlet.2018.06.007) 40-90% decreases in blood lead, urinary mercury, and pentachlorophenol over the past several decades, providing direct empirical confirmation that regulatory interventions (lead phase-out from gasoline, restrictions on amalgam dental fillings, pentachlorophenol bans) actually translated into reduced human exposure.

The program has also served as an early warning system. When researchers applied newer analytical methods to archived samples, they detected [rising trends in polybrominated diphenyl ethers (PBDEs)](https://doi.org/10.1016/j.ijheh.2007.01.036) years before these flame retardants became a major regulatory concern. Similar retrospective analyses tracked the rise and fall of PFOS and PFOA in the German population, documenting both the increase during peak production years and the subsequent decline following manufacturing restrictions.

Perhaps most valuably, the ESBHum has revealed cases where exposure estimates based on production and consumption data diverged significantly from actual measured body burden. This finding reinforces the central argument of this essay: modeling is not enough. Without empirical verification through direct measurement, regulators may systematically over- or underestimate the chemicals actually accumulating in human bodies.

### Limitations and Lessons

The ESBHum is not a perfect model for tissue surveillance. Its reliance on biological fluids rather than solid organ tissue means it shares the pharmacokinetic limitations of NHANES, missing the cumulative burden of lipophilic compounds sequestered in adipose tissue, metals deposited in bone, or emerging contaminants like microplastics accumulating in the brain. Interestingly, the program's early pilot phase (1978-1983) did include autopsy material such as liver and adipose tissue, but this component was not sustained into the permanent program.

The sampling population also introduces bias: healthy university students aged 20-29 are not representative of the broader population, particularly children, the elderly, or those with occupational exposures. Germany addresses this limitation through a complementary instrument, the German Environmental Survey (GerES), which provides population-representative cross-sectional data that can be compared against ESBHum time trends.

What the ESBHum demonstrates is institutional feasibility. A sustained, government-funded biomonitoring archive can operate for decades when properly resourced and when its outputs are explicitly connected to regulatory decision-making. The program currently operates under the Federal Environment Agency with laboratory work conducted by Fraunhofer IBMT, providing a clear institutional home and stable funding stream. This stands in sharp contrast to NHATS, which was allowed to lapse despite scientific recommendations for its continuation.

For those envisioning a more ambitious tissue surveillance program that includes solid organs, the ESBHum offers both encouragement and caution. Encouragement because it proves that multi-decade human specimen banking is achievable. Caution because even this well-established program has not managed to sustain its early tissue-banking component, suggesting that the logistical and ethical complexities of organ tissue collection may require dedicated infrastructure beyond what a fluids-focused biobank can provide.

## Child Health and Mortality Study (CHAMPS) (2015 - Present)

Established in 2015 by the Bill & Melinda Gates Foundation (with sites becoming operational in 2016-2017), CHAMPS is a global health surveillance network dedicated to determining the definitive causes of death in children under five and stillbirths in high-mortality regions. As of 2024, the network operates in nine countries: Bangladesh, Ethiopia, Kenya, Mali, Mozambique, Nigeria, Pakistan, Sierra Leone, and South Africa.

Moving beyond standard verbal autopsies, CHAMPS utilizes a "ground truth" protocol that combines clinical records with Minimally Invasive Tissue Sampling (MITS), which are then reviewed by a multidisciplinary panel to assign a precise cause of death. While the network currently prioritizes infectious diseases and malnutrition, its established infrastructure for collecting and banking solid organs seems to me like a potential candidate for future environmental toxicological surveillance.