---
title: "What Gets Measured Gets Improved"
date: 2025-12-24T17:19:54-05:00
draft: true
---

There's a principle that circulates through management consulting, public health, and behavioral psychology with almost axiomatic authority: *what gets measured gets improved*. The phrase is often attributed to Peter Drucker, though like many management aphorisms, its origins are murky. Lord Kelvin offered an earlier version in the 19th century when he claimed that without measurement, improvement becomes impossible.

The question I want to explore here is whether this principle holds up to scrutiny, and if so, what mechanisms might explain it. Because if measurement alone can drive improvement, that's a remarkably powerful lever. And if it can't, then we're collectively investing enormous resources in dashboards, KPIs, and tracking systems that may be producing more noise than signal.

## The Evidence Base

The research literature offers some striking examples. In healthcare, the U.S. Agency for Healthcare Research and Quality found that [publicly reported quality measures tend to improve over time](https://effectivehealthcare.ahrq.gov/sites/default/files/pdf/public-reporting-quality-improvement_research.pdf), with providers responding to data transparency through quality improvement efforts. New York State's public reporting of cardiac surgery mortality rates in the 1990s precipitated faster declines in risk-adjusted mortality compared to the rest of the country. Similar patterns emerged when the UK introduced cardiac surgery report cards.

At the individual level, the effects can be equally pronounced. A meta-analysis of pedometer use found that people who tracked their steps increased their daily activity by about 27% over baseline. Daily self-weighing trials show participants [losing significantly more weight](https://pmc.ncbi.nlm.nih.gov/articles/PMC4380831/) than those weighing weekly or less. The mere act of being asked about health behaviors in surveys appears to [increase the likelihood of those behaviors occurring](https://pmc.ncbi.nlm.nih.gov/articles/PMC12064450/), a phenomenon researchers call the "question-behavior effect" or "mere-measurement effect."

General Electric's Six Sigma program offers a corporate example: by rigorously measuring defects and process variability, the company reportedly saved $12 billion over five years. The Millennium Development Goals provide a population-scale case, with researchers estimating that [21 to 29 million additional lives were saved](https://www.theguardian.com/global-development-professionals-network/2017/mar/30/how-successful-were-the-millennium-development-goals) during the MDG period compared to prior trend lines, attributable in part to the focused attention and resources guided by measured targets.

## Candidate Mechanisms

If measurement does drive improvement, what's actually happening? Several mechanisms seem plausible, and they likely operate in combination.

### Attention and salience

The most straightforward explanation is attentional. Measuring something makes it visible, and visibility directs cognitive resources. A hospital that tracks infection rates will inevitably think more about infection control. A person who weighs themselves daily will find weight management more consistently present in their decision-making. The metric becomes a persistent prompt.

### Feedback loops

Measurement creates the possibility of feedback, and feedback enables learning. Without data, you're operating in the dark, unable to distinguish interventions that work from those that don't. A UK randomized trial on hand hygiene compliance found that wards receiving [regular feedback on their washing rates](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0041617) showed 10 to 18 percentage points greater improvement than those without such feedback. The measurement itself wasn't sufficient; the feedback loop closed the circuit.

### Accountability and social pressure

Public measurement adds another dimension: social accountability. When cardiac surgery outcomes became publicly visible, hospitals couldn't hide poor performance. The reputational stakes created pressure to improve. This is essentially the [Hawthorne effect](https://www.simplypsychology.org/hawthorne-effect.html) scaled up. In the original experiments, workers' productivity increased simply because they knew they were being observed. The awareness of measurement, even without any intervention, was sufficient to alter behavior.

### Goal concretization

Abstract aspirations are difficult to act on. "Be healthier" provides no clear path forward. "Walk 10,000 steps today" does. Measurement transforms vague intentions into specific targets, and specific targets allow for planning, progress tracking, and the psychological satisfaction of achievement. The Millennium Development Goals worked in part because they converted diffuse humanitarian aims into concrete indicators that could be monitored and addressed.

## Caveats and Failure Modes

The picture isn't uniformly positive, and the caveats matter.

First, the Hawthorne effect's productivity gains were temporary. Once the special attention ceased, output returned to baseline. This suggests that measurement effects may require sustained engagement rather than one-time assessment.

Second, Goodhart's Law looms over any measurement regime: "When a measure becomes a target, it ceases to be a good measure." Organizations optimizing for metrics can game them, hitting the numbers while undermining the underlying goals. A hospital might improve its reported mortality rate by avoiding high-risk patients rather than by improving care quality. The metric improves; the thing it was supposed to measure does not.

Third, measurement has costs. The cognitive overhead of tracking, the administrative burden of reporting systems, the potential for measurement anxiety. Not everything worth improving is measurable, and not everything measurable is worth improving. As management thinkers have cautioned, what gets measured gets managed, even if it's not important.

## Open Questions

Several questions seem worth pursuing further:

Under what conditions does the measurement effect persist versus fade? The Hawthorne experiments suggest impermanence, but daily self-weighing trials show sustained effects over months. What distinguishes the two contexts?

How does the granularity of measurement affect outcomes? Is daily tracking superior to weekly? Is there a point of diminishing returns or even negative effects from too-frequent measurement?

What role does agency play? The effect seems strongest when the person being measured is also the one who can act on the data. How does the measurement-improvement link change when measurement and action are separated, as in many organizational contexts?

Can we predict which metrics will be robust to Goodhart's Law and which will be gamed?

## Provisional Conclusions

The evidence suggests that measurement can indeed drive improvement, but the relationship is contingent rather than automatic. The mechanism appears to involve attention direction, feedback enablement, accountability creation, and goal specification. The effect is strongest when measurement is coupled with feedback, when the person measured has agency over the outcome, and when the metric genuinely reflects the underlying goal.

What gets measured may get improved, but not inevitably. The measurement system itself requires design attention: choosing metrics that resist gaming, ensuring feedback reaches those who can act, maintaining engagement over time, and accepting that some important things will remain unmeasured. The principle holds, but with conditions.